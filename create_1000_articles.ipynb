{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11de3e50",
   "metadata": {},
   "source": [
    "# Create a thousand articles for one topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d11fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio-client 0.7.1 requires websockets<12.0,>=10.0, but you have websockets 15.0.1 which is incompatible.\n",
      "llama-index-readers-file 0.4.9 requires pandas<2.3.0, but you have pandas 2.3.0 which is incompatible.\n",
      "selenium 4.33.0 requires urllib3[socks]~=2.4.0, but you have urllib3 1.26.20 which is incompatible.\n",
      "tensorboard 2.12.3 requires google-auth-oauthlib<1.1,>=0.5, but you have google-auth-oauthlib 1.2.2 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-genai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b88fb253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070d6d2",
   "metadata": {},
   "source": [
    "# Make sure you create a .env.local file and put your GEMINI_API_KEY in there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a24db55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\".env.local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5131d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client(api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d85715",
   "metadata": {},
   "source": [
    "# Generate topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69629c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts/topic_generation.txt\", \"r\") as f:\n",
    "    topic_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a994bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Model deployment\", \"Data pipelines\", \"MLOps principles\", \"Active learning\", \"Online learning\", \"Transfer learning\", \"Set theory\", \"Vector space\", \"Convex optimization\", \"Causal inference\"]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=topic_prompt,\n",
    ")\n",
    "topics = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e57cce",
   "metadata": {},
   "source": [
    "# Save the list of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66caa738",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_list = json.loads(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c60ceb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles/machine_learning/list_of_topics.json\", \"w\") as f:\n",
    "    json.dump(json.loads(topics), f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08cb12d",
   "metadata": {},
   "source": [
    "# Generate Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a4fe545",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts/article_creation.text\", \"r\") as f:\n",
    "    prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7cb135",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_str = \"\"\"```\\njson {\"topics\": topics_list, \"num_articles\": 1000}\\n```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e031206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"topics\": topics_list, \"num_articles\": 1000}'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_str.split('```json')[1].split('```')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b872aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_article(topic, article, idx):\n",
    "    # lowecase all topics, split by space, and join with underscore\n",
    "    # example: \"Machine Learning\" -> \"machine_learning\"\n",
    "    title = \"_\".join(topic.lower().split(\" \"))\n",
    "    with open(f\"articles/machine_learning/{title}.json\", \"w\") as f:\n",
    "        json.dump(article, f, indent=2)\n",
    "    print(f\"Saved article {idx + 1} for topic: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "279c0178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model deployment',\n",
       " 'Data pipelines',\n",
       " 'MLOps principles',\n",
       " 'Active learning',\n",
       " 'Online learning']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470ed3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec311189270406683cbd6f3cf847cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating article for topic: Model deployment\n",
      "Response is not a valid JSON, trying to fix it.\n",
      "Still not a valid JSON, redo generation.\n",
      "Generating article for topic: Model deployment\n",
      "Response is not a valid JSON, trying to fix it.\n",
      "Still not a valid JSON, redo generation.\n",
      "Generating article for topic: Model deployment\n",
      "Saved article 1 for topic: model_deployment\n",
      "Generating article for topic: Data pipelines\n",
      "Response is not a valid JSON, trying to fix it.\n",
      "Response is not a valid JSON, redo generation.\n",
      "Generating article for topic: Data pipelines\n",
      "Saved article 2 for topic: data_pipelines\n",
      "Generating article for topic: MLOps principles\n",
      "Saved article 3 for topic: mlops_principles\n",
      "Generating article for topic: Active learning\n",
      "Response is not a valid JSON, trying to fix it.\n",
      "Still not a valid JSON, redo generation.\n",
      "Generating article for topic: Active learning\n",
      "Saved article 4 for topic: active_learning\n",
      "Generating article for topic: Online learning\n",
      "Saved article 5 for topic: online_learning\n"
     ]
    }
   ],
   "source": [
    "# A while loop to generate articles for each topic is bettern than a for loop\n",
    "# because the LLM can generate non-json responses, and we can handle that case\n",
    "is_done = False\n",
    "index = 0\n",
    "# This could be long. Reduce the number of topics to generate articles for and do it in batches.\n",
    "shortened_topics = topics_list[0:5]  # Adjust the range as needed\n",
    "total = len(shortened_topics)  # Adjust the range as needed\n",
    "pbar = tqdm(total=total)\n",
    "while not is_done:\n",
    "    topic = shortened_topics[index]\n",
    "    final_prompt = prompt.format(topic=topic)\n",
    "    print(f\"Generating article for topic: {topic}\")\n",
    "    article_response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\", contents=final_prompt,\n",
    "    )\n",
    "    article_text = article_response.text\n",
    "    try:\n",
    "        article_dict = json.loads(article_text)\n",
    "        save_article(shortened_topics[index], article_dict, index)\n",
    "        index += 1\n",
    "        pbar.update(total - pbar.n)\n",
    "    except json.JSONDecodeError:\n",
    "        # two options now.\n",
    "        # 1. The response is not a valid JSON, we can try to fix it.\n",
    "        print(\"Response is not a valid JSON, trying to fix it.\")\n",
    "        if '```json' in article_text:\n",
    "            # remove the ````json` and ` ``` ` from the response\n",
    "            clean_text = article_text.split('```json')[1].split('```')[0].strip()\n",
    "            try:\n",
    "                article_dict = json.loads(clean_text)\n",
    "                save_article(shortened_topics[index], article_dict, index)\n",
    "                index += 1\n",
    "                pbar.update(total - pbar.n)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Still not a valid JSON, redo generation.\")\n",
    "                # If it still fails, we can just skip this topic and move to the next one.\n",
    "        else:\n",
    "            print(\"Response is not a valid JSON, redo generation.\")\n",
    "        # 2. The response is not a valid JSON, we can reset and have it generate again.\n",
    "    if index >= len(shortened_topics):\n",
    "        is_done = True\n",
    "pbar.update(total - pbar.n)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3625d5e",
   "metadata": {},
   "source": [
    "# Save all the articles in a json format for further processing later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6f680c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLOps principles'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac0f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, article in enumerate(list_articles):\n",
    "    # lowecase all topics, split by space, and join with underscore\n",
    "    # example: \"Machine Learning\" -> \"machine_learning\"\n",
    "    title = \"_\".join(topics_list[idx].lower().split(\" \"))\n",
    "    with open(f\"articles/machine_learning/{title}.json\", \"w\") as f:\n",
    "        json.dump(article, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f8b99e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
